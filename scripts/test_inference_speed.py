#!/usr/bin/env python3
"""Test raw inference speed of YOLO models."""

import time
import numpy as np
import cv2
from pathlib import Path
import sys
import os

# Add the detection service to the path
sys.path.insert(0, str(Path(__file__).parent.parent / "detection-service"))

try:
    from detectsvc.accel.onnx_cpu import ONNXCPURunner
    from detectsvc.registry import registry
except ImportError as e:
    print(f"âŒ Could not import detection modules: {e}")
    print("Make sure you're running from the intrusion-suite directory")
    sys.exit(1)

def create_test_frame(width=640, height=640):
    """Create a test frame for inference."""
    # Create a synthetic test image (random noise + some shapes)
    frame = np.random.randint(0, 255, (height, width, 3), dtype=np.uint8)
    
    # Add some shapes to make it more realistic
    cv2.rectangle(frame, (100, 100), (200, 200), (255, 0, 0), -1)
    cv2.circle(frame, (400, 400), 50, (0, 255, 0), -1)
    cv2.rectangle(frame, (300, 100), (500, 300), (0, 0, 255), 2)
    
    return frame

def benchmark_model(model_path, model_name, num_frames=100):
    """Benchmark a single model."""
    print(f"\nğŸ§ª Testing {model_name}")
    print(f"ğŸ“ Model: {model_path}")
    
    if not Path(model_path).exists():
        print(f"âŒ Model file not found: {model_path}")
        return None
    
    try:
        # Load model
        print("â³ Loading model...")
        runner = ONNXCPURunner()
        runner.load(Path(model_path))
        
        # Set dummy class names
        runner.class_names = ["class_0", "class_1", "class_2"]
        
        # Create test frame
        test_frame = create_test_frame()
        print(f"ğŸ“ Test frame: {test_frame.shape}")
        
        # Warmup (first inference is usually slower)
        print("ğŸ”¥ Warming up...")
        for _ in range(5):
            runner.infer(test_frame)
        
        # Benchmark
        print(f"âš¡ Running {num_frames} inferences...")
        start_time = time.time()
        
        for i in range(num_frames):
            detections = runner.infer(test_frame)
            if i == 0:
                print(f"ğŸ¯ First inference: {len(detections)} detections")
        
        end_time = time.time()
        elapsed = end_time - start_time
        fps = num_frames / elapsed
        
        print(f"âœ… Results:")
        print(f"   ğŸš€ FPS: {fps:.1f}")
        print(f"   â±ï¸  Total time: {elapsed:.2f}s")
        print(f"   ğŸ“Š Avg time per frame: {(elapsed/num_frames)*1000:.1f}ms")
        
        return fps
        
    except Exception as e:
        print(f"âŒ Error benchmarking {model_name}: {e}")
        return None

def main():
    """Run inference speed test."""
    print("ğŸƒâ€â™‚ï¸ YOLO Inference Speed Test")
    print("="*50)
    
    # Find model directory
    models_dir = Path(__file__).parent.parent / "models"
    if not models_dir.exists():
        print(f"âŒ Models directory not found: {models_dir}")
        return
    
    # Find available models
    model_files = list(models_dir.glob("*.onnx"))
    if not model_files:
        print(f"âŒ No ONNX models found in {models_dir}")
        return
    
    print(f"ğŸ“‚ Found {len(model_files)} models in {models_dir}")
    
    results = {}
    
    # Test each model
    for model_file in model_files:
        model_name = model_file.stem
        fps = benchmark_model(str(model_file), model_name)
        if fps:
            results[model_name] = fps
    
    # Summary
    print("\n" + "="*50)
    print("ğŸ“Š PERFORMANCE SUMMARY")
    print("="*50)
    
    if results:
        for model_name, fps in sorted(results.items(), key=lambda x: x[1], reverse=True):
            print(f"ğŸ¥‡ {model_name:<20} {fps:>8.1f} FPS")
        
        avg_fps = sum(results.values()) / len(results)
        best_fps = max(results.values())
        
        print(f"\nğŸ“ˆ Average FPS: {avg_fps:.1f}")
        print(f"ğŸš€ Best FPS: {best_fps:.1f}")
        
        if best_fps > 60:
            print("ğŸ‰ EXCELLENT! Your system achieves native-script performance!")
        elif best_fps > 30:
            print("âœ… GOOD! Solid performance for real-time detection.")
        else:
            print("âš ï¸  Consider optimizing or using a smaller model.")
    else:
        print("âŒ No successful benchmarks")
    
    print("\nğŸ’¡ Tips:")
    print("   - Higher FPS = better performance")
    print("   - Test with real camera feed might be different")
    print("   - GPU acceleration would be even faster")

if __name__ == "__main__":
    main()
